{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5a7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from albumentations import HorizontalFlip, GridDistortion, OpticalDistortion, ChannelShuffle, CoarseDropout, CenterCrop, Crop, Rotate\n",
    "from PIL import Image \n",
    "\n",
    "# def load_data(path, split=0.2):\n",
    "# #     path = r\"\"\n",
    "#     X = sorted(glob(os.path.join(path, \"images\", \"*.jpg\")))\n",
    "#     Y = sorted(glob(os.path.join(path, \"masks\", \"*.png\")))\n",
    "#     train_x, test_x = train_test_split(X, test_size=0.2, random_state=42)\n",
    "#     train_y, test_y = train_test_split(Y, test_size=0.2, random_state=42)\n",
    "#     return (train_x, train_y), (test_x, test_y)\n",
    "# p = r\"/home/jai201/Jai_Minor/semantic_resized\"\n",
    "# load_data(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59c392",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d423c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def augment_data(images, masks, save_path, augment=True):\n",
    "    H = 512\n",
    "    W = 512\n",
    "    for x, y in tqdm(zip(images, masks), total=len(images)):\n",
    "        \"\"\" Extract the name \"\"\"\n",
    "        name = os.path.splitext(os.path.basename(x))[0]\n",
    "        \n",
    "\n",
    "        \"\"\" Reading the image and mask \"\"\"\n",
    "        c = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        d = cv2.imread(y, cv2.IMREAD_COLOR)\n",
    "        x = Image.open(x)\n",
    "        y = Image.open(y)\n",
    "        \n",
    "#         x = np.array(a)\n",
    "#         y = np.array(b)\n",
    "        \"\"\" Augmentation \"\"\"\n",
    "        if augment == True:\n",
    "            \n",
    "            aug = HorizontalFlip(p=1.0)\n",
    "            augmented = aug(image=np.array(x), mask=np.array(y))\n",
    "            x1 = augmented[\"image\"]\n",
    "            y1 = augmented[\"mask\"]\n",
    "\n",
    "            x2 = cv2.cvtColor(c, cv2.COLOR_RGB2GRAY)\n",
    "            y2 = d\n",
    "\n",
    "            aug = ChannelShuffle(p=1)\n",
    "            augmented = aug(image=np.array(x), mask=np.array(y))\n",
    "            x3 = augmented['image']\n",
    "            y3 = augmented['mask']\n",
    "\n",
    "            aug = CoarseDropout(p=1, min_holes=3, max_holes=10, max_height=32, max_width=32)\n",
    "            augmented = aug(image=np.array(x), mask=np.array(y))\n",
    "            x4 = augmented['image']\n",
    "            y4 = augmented['mask']\n",
    "\n",
    "            aug = Rotate(limit=45, p=1.0)\n",
    "            augmented = aug(image=np.array(x), mask=np.array(y))\n",
    "            x5 = augmented[\"image\"]\n",
    "            y5 = augmented[\"mask\"]\n",
    "\n",
    "            X = [x, x1, x2, x3, x4, x5]\n",
    "            Y = [y, y1, y2, y3, y4, y5]\n",
    "            index = 0\n",
    "            for i, m in zip(X, Y):\n",
    "                try:\n",
    "                    \"\"\" Center Cropping \"\"\"\n",
    "                    aug = CenterCrop(H, W, p=1.0)\n",
    "                    augmented = aug(image=i, mask=m)\n",
    "                    i = augmented[\"image\"]\n",
    "                    m = augmented[\"mask\"]\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "#                     basewidth = 512\n",
    "# #                     img = Image.open('somepic.jpg')\n",
    "#                     wpercent = (basewidth/float(Image.fromarray(augmented[\"image\"]).size[0]))\n",
    "#                     hsize = int((float(Image.fromarray(augmented[\"image\"]).size[1])*float(wpercent)))\n",
    "#                     im1 = Image.fromarray(augmented[\"image\"]).resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "#                     im2 = Image.fromarray(augmented[\"mask\"]).resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "#                     size = 512 , 512\n",
    "                    \n",
    "#                     i = im.thumbnail(size, Image.ANTIALIAS)\n",
    "#                     m = cv2.resize(i , (W,H))\n",
    "                    \n",
    "\n",
    "                tmp_image_name = f\"{name}_{index}\"\n",
    "                tmp_mask_name = f\"{name}_{index}\"\n",
    "\n",
    "                image_path = os.path.join(save_path, \"image\", tmp_image_name)\n",
    "                mask_path = os.path.join(save_path, \"mask\", tmp_mask_name)\n",
    "\n",
    "    #             cv2.imwrite(image_path, i)\n",
    "    #             cv2.imwrite(mask_path, m)\n",
    "                im1 = Image.fromarray(augmented[\"image\"])\n",
    "                im2 = Image.fromarray(augmented[\"mask\"])\n",
    "                im1= im1.save(r\"D:\\Minor\\leaf\\new_data\\train\\image/\"+ f'{str(name)}_{index}' + '.png')\n",
    "                im2 = im2.save(r\"D:\\Minor\\leaf\\new_data\\train\\mask/\"+ f'{str(name)}_{index}'+'.png')\n",
    "\n",
    "                index += 1\n",
    "\n",
    "        else:\n",
    "            X = [x]\n",
    "            Y = [y]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "# /    np.random.seed(42)\n",
    "\n",
    "# #     \"\"\" Load the dataset \"\"\"\n",
    "#     data_path = r\"D:\\Minor\\leaf\\data\"\n",
    "#     (train_x, train_y), (test_x, test_y) = load_data(data_path)\n",
    "\n",
    "#     print(f\"Train:\\t {len(train_x)} - {len(train_y)}\")\n",
    "#     print(f\"Test:\\t {len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "    \"\"\" Data augmentation \"\"\"\n",
    "#     augment_data(train_x, train_y, \"D:\\Minor\\leaf\\new_data\\train\", augment=True)\n",
    "#     augment_data(test_x, test_y, \"D:\\Minor\\leaf\\new_data\\test\", augment=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa0d8fa",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040ca378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 180, 120, 256), dtype=tf.float32, name=None), name='up_sampling2d_1/resize/ResizeBilinear:0', description=\"created by layer 'up_sampling2d_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 180, 120, 256), dtype=tf.float32, name=None), name='activation_6/Relu:0', description=\"created by layer 'activation_6'\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Reshape, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import tensorflow as tf\n",
    "\n",
    "def SqueezeAndExcite(inputs, ratio=8):\n",
    "    init = inputs\n",
    "    filters = init.shape[-1]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    x = init * se\n",
    "    return x\n",
    "\n",
    "def ASPP(inputs):\n",
    "    \"\"\" Image Pooling \"\"\"\n",
    "    shape = inputs.shape\n",
    "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(inputs)\n",
    "    y1 = Conv2D(256, 1, padding=\"same\", use_bias=False)(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation(\"relu\")(y1)\n",
    "    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n",
    "\n",
    "    \"\"\" 1x1 conv \"\"\"\n",
    "    y2 = Conv2D(256, 1, padding=\"same\", use_bias=False)(inputs)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y2 = Activation(\"relu\")(y2)\n",
    "\n",
    "    \"\"\" 3x3 conv rate=6 \"\"\"\n",
    "    y3 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=6)(inputs)\n",
    "    y3 = BatchNormalization()(y3)\n",
    "    y3 = Activation(\"relu\")(y3)\n",
    "\n",
    "    \"\"\" 3x3 conv rate=12 \"\"\"\n",
    "    y4 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=12)(inputs)\n",
    "    y4 = BatchNormalization()(y4)\n",
    "    y4 = Activation(\"relu\")(y4)\n",
    "\n",
    "    \"\"\" 3x3 conv rate=18 \"\"\"\n",
    "    y5 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=18)(inputs)\n",
    "    y5 = BatchNormalization()(y5)\n",
    "    y5 = Activation(\"relu\")(y5)\n",
    "\n",
    "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
    "    y = Conv2D(256, 1, padding=\"same\", use_bias=False)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def deeplabv3_plus(shape):\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = Input(shape)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    encoder = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
    "\n",
    "    image_features = encoder.get_layer(\"conv4_block6_out\").output\n",
    "    x_a = ASPP(image_features)\n",
    "    x_a = UpSampling2D((4, 4), interpolation=\"bilinear\")(x_a)\n",
    "#     x_a = tf.keras.layers.Reshape((270, 180,3))\n",
    "    print(x_a)                              \n",
    "    \n",
    "    \n",
    "    x_b = encoder.get_layer(\"conv2_block2_out\").output\n",
    "    x_b = Conv2D(filters=256, kernel_size=1, padding='same', use_bias=False)(x_b)\n",
    "    x_b = BatchNormalization()(x_b)\n",
    "    x_b = Activation('relu')(x_b)\n",
    "    print(x_b)\n",
    "#     return 0\n",
    "\n",
    "    x = Concatenate()([x_a, x_b])\n",
    "    x = SqueezeAndExcite(x)\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SqueezeAndExcite(x)\n",
    "\n",
    "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
    "    x = Conv2D(1, 1)(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = deeplabv3_plus((720,480,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7c093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    def f(y_true, y_pred):\n",
    "        intersection = (y_true * y_pred).sum()\n",
    "        union = y_true.sum() + y_pred.sum() - intersection\n",
    "        x = (intersection + 1e-15) / (union + 1e-15)\n",
    "        x = x.astype(np.float32)\n",
    "        return x\n",
    "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
    "\n",
    "smooth = 1e-15\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47f25f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65318c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 320 - 320\n",
      "Valid: 80 - 80\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 120, 180, 256), dtype=tf.float32, name=None), name='up_sampling2d_7/resize/ResizeBilinear:0', description=\"created by layer 'up_sampling2d_7'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 120, 180, 256), dtype=tf.float32, name=None), name='activation_26/Relu:0', description=\"created by layer 'activation_26'\")\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "\n",
    "\n",
    "\"\"\" Global parameters \"\"\"\n",
    "W = 720\n",
    "H = 480\n",
    "\n",
    "\"\"\" Creating a directory \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def shuffling(x, y):\n",
    "    x, y = shuffle(x, y, random_state=42)\n",
    "    return x, y\n",
    "\n",
    "def load_data(path):\n",
    "    x = sorted(glob(os.path.join(path, \"images\", \"*jpg\")))\n",
    "    y = sorted(glob(os.path.join(path, \"masks\", \"*png\")))\n",
    "    return x, y\n",
    "\n",
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    return x\n",
    "\n",
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
    "    x.set_shape([H, W, 3])\n",
    "    y.set_shape([H, W, 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(X, Y, batch=1):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(10)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    \"\"\" Directory for storing files \"\"\"\n",
    "    create_dir(\"files\")\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    batch_size = 40\n",
    "    lr = 1e-4\n",
    "    num_epochs = 10\n",
    "    model_path = os.path.join(\"/home/jai201/Jai_Minor/\", \"deeplab_model.h5\")\n",
    "    csv_path = os.path.join(\"/home/jai201/Jai_Minor/\", \"data.csv\")\n",
    "\n",
    "    \"\"\" Dataset \"\"\"\n",
    "    dataset_path = r\"/home/jai201/Jai_Minor/semantic_resized/split2\"\n",
    "    train_path = os.path.join(dataset_path, \"train\")\n",
    "    valid_path = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "    train_x, train_y = load_data(train_path)\n",
    "    train_x, train_y = shuffling(train_x, train_y)\n",
    "    valid_x, valid_y = load_data(valid_path)\n",
    "\n",
    "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
    "\n",
    "    train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
    "    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n",
    "\n",
    "#     \"\"\" Model \"\"\"\n",
    "    model = deeplabv3_plus((H,W, 3))\n",
    "    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef, iou, Recall(), Precision()])\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
    "        CSVLogger(csv_path),\n",
    "        TensorBoard(),\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=False),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=valid_dataset,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b8d83",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "# from metrics import dice_loss, dice_coef, iou\n",
    "# from train import load_data\n",
    "\n",
    "\"\"\" Global parameters \"\"\"\n",
    "H = 512\n",
    "W = 512\n",
    "\n",
    "\"\"\" Creating a directory \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def save_results(image, mask, y_pred, save_image_path):\n",
    "    ## i - m - yp - yp*i\n",
    "    line = np.ones((H, 10, 3)) * 128\n",
    "\n",
    "    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
    "    mask = mask * 255\n",
    "\n",
    "    y_pred = np.expand_dims(y_pred, axis=-1)    ## (512, 512, 1)\n",
    "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1)  ## (512, 512, 3)\n",
    "\n",
    "    masked_image = image * y_pred\n",
    "    y_pred = y_pred * 255\n",
    "\n",
    "    cat_images = np.concatenate([image, line, mask, line, y_pred, line, masked_image], axis=1)\n",
    "    cv2.imwrite(save_image_path, cat_images)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "#     np.random.seed(42)\n",
    "#     tf.random.set_seed(42)\n",
    "\n",
    "#     \"\"\" Directory for storing files \"\"\"\n",
    "#     create_dir(\"results\")\n",
    "\n",
    "#     \"\"\" Loading model \"\"\"\n",
    "#     with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef, 'dice_loss': dice_loss}):\n",
    "#         model = tf.keras.models.load_model(\"/home/jai201/Jai_Minor/model.h5\")\n",
    "\n",
    "#     \"\"\" Load the dataset \"\"\"\n",
    "#     dataset_path = \"/home/jai201/Jai_Minor/new_data\"\n",
    "#     valid_path = os.path.join(dataset_path, \"test\")\n",
    "#     test_x, test_y = load_data(valid_path)\n",
    "#     print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "#     \"\"\" Evaluation and Prediction \"\"\"\n",
    "#     SCORE = []\n",
    "#     for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
    "#         \"\"\" Extract the name \"\"\"\n",
    "#         name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "#         \"\"\" Reading the image \"\"\"\n",
    "#         image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "#         x = image/255.0\n",
    "#         x = np.expand_dims(x, axis=0)\n",
    "\n",
    "#         \"\"\" Reading the mask \"\"\"\n",
    "#         mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#         \"\"\" Prediction \"\"\"\n",
    "#         y_pred = model.predict(x)[0]\n",
    "#         y_pred = np.squeeze(y_pred, axis=-1)\n",
    "#         y_pred = y_pred > 0.5\n",
    "#         y_pred = y_pred.astype(np.int32)\n",
    "\n",
    "#         \"\"\" Saving the prediction \"\"\"\n",
    "#         save_image_path = f\"/home/jai201/Jai_Minor/results/{name}.png\"\n",
    "#         save_results(image, mask, y_pred, save_image_path)\n",
    "\n",
    "#         \"\"\" Flatten the array \"\"\"\n",
    "#         mask = mask.flatten()\n",
    "#         y_pred = y_pred.flatten()\n",
    "\n",
    "#         \"\"\" Calculating the metrics values \"\"\"\n",
    "#         acc_value = accuracy_score(mask, y_pred)\n",
    "#         f1_value = f1_score(mask, y_pred, pos_label='positive',average='micro')\n",
    "#         jac_value = jaccard_score(mask, y_pred, pos_label='positive',average='micro')\n",
    "#         recall_value = recall_score(mask, y_pred, pos_label='positive',average='micro')\n",
    "#         precision_value = precision_score(mask, y_pred, pos_label='positive',average='micro')\n",
    "#         SCORE.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n",
    "\n",
    "#     \"\"\" Metrics values \"\"\"\n",
    "#     score = [s[1:]for s in SCORE]\n",
    "#     score = np.mean(score, axis=0)\n",
    "#     print(f\"Accuracy: {score[0]:0.5f}\")\n",
    "#     print(f\"F1: {score[1]:0.5f}\")\n",
    "#     print(f\"Jaccard: {score[2]:0.5f}\")\n",
    "#     print(f\"Recall: {score[3]:0.5f}\")\n",
    "#     print(f\"Precision: {score[4]:0.5f}\")\n",
    "\n",
    "#     df = pd.DataFrame(SCORE, columns=[\"Image\", \"Accuracy\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n",
    "#     df.to_csv(\"files/score.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a656cda",
   "metadata": {},
   "source": [
    "# Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "\n",
    "\"\"\" Global parameters \"\"\"\n",
    "H = 512\n",
    "W = 512\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    \"\"\" Directory for storing files \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\" Loading model \"\"\"\n",
    "    with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef, 'dice_loss': dice_loss}):\n",
    "        model = tf.keras.models.load_model(\"/home/jai201/Jai_Minor/model.h5\")\n",
    "\n",
    "    \"\"\" Load the dataset \"\"\"\n",
    "    data_x = glob(\"/home/jai201/Downloads/Leaf_Spot_Horizontal_15.jpg\")\n",
    "\n",
    "    for path in tqdm(data_x, total=len(data_x)):\n",
    "        \"\"\" Extracting name \"\"\"\n",
    "        name = path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        \"\"\" Reading the image \"\"\"\n",
    "        image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        h, w, _ = image.shape\n",
    "        x = cv2.resize(image, (W, H))\n",
    "        x = x/255.0\n",
    "        x = x.astype(np.float32)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "\n",
    "        \"\"\" Prediction \"\"\"\n",
    "        y = model.predict(x)[0]\n",
    "        y = cv2.resize(y, (w, h))\n",
    "        y = np.expand_dims(y, axis=-1)\n",
    "\n",
    "        \"\"\" Save the image \"\"\"\n",
    "        masked_image = image * y\n",
    "        line = np.ones((h, 10, 3)) * 128\n",
    "        cat_images = np.concatenate([image, line, masked_image], axis=1)\n",
    "        cv2.imwrite(f\"/home/jai201/Jai_Minor/{name}.png\", cat_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f227db38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
